{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# LDA_incl_pyLDAvis.py\n",
    "#\n",
    "# Description:\n",
    "# This script performs Latent Dirichlet Allocation (LDA) on a clean corpus.\n",
    "#\n",
    "# Version: 0.3\n",
    "# Date Created: 08/28/2019\n",
    "# Last Modified: 06/03/2021\n",
    "#####################################################################################\n",
    "#\n",
    "# Note from 06/03/2021:\n",
    "# Package pyLDAvis v.3.3.0 renamed \"pyLDAvis.gensim\" to \"pyLDAvis.gensim_models\"\n",
    "#\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL VARIABLES ###\n",
    "\n",
    "PROJECT_FOLDER = \"C://Users/kathleen.trinh/Documents/Nissan/LEAF/YouTube/LDA/\"\n",
    "FILENAME = \"Nissan_LEAF_YouTube_ALL_CLEAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "\n",
    "# Built-in Libraries\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET-UP ###\n",
    "\n",
    "# Load .csv file and put data in dataframe\n",
    "df = pd.read_csv((PROJECT_FOLDER + FILENAME + \".csv\"), header=0, usecols=[\"my_comment_id\", \"clean_body_no_stopwords\"], encoding='utf-8')\n",
    "\n",
    "# Rename columns\n",
    "#df.columns = [\"My Comment ID\", \"Clean Body no stopwords\"]\n",
    "\n",
    "# Print the dataframe's dimentsions\n",
    "print(\"Dataframe Dimensions: {} Rows, {} Columns\".format(*df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample of 5 elements from the dataframe\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Randomly sample 10,000 rows from the data\n",
    "random.seed(1)\n",
    "indices = df.index.values.tolist()\n",
    "\n",
    "random_10000 = random.sample(indices, 10000)\n",
    "random_10000[:5]\n",
    "\n",
    "# Subset the imported data on the selected 2500 indices\n",
    "train_df = df.loc[random_10000, :]\n",
    "train_df = train_df.reset_index(drop = True)\n",
    "train_df.head(2)\n",
    "\n",
    "# Print a sample of 50 elements from the training dataframe\n",
    "train_df.sample(50)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE CORPUS ###\n",
    "\n",
    "# Make a new corpus called text_data (type: list of string)\n",
    "# where each string corresponds to a document's \"Clean Body no stopwords\" field\n",
    "#text_data = train_df[\"Clean Body no stopwords\"].tolist()\n",
    "\n",
    "# OR use the entire corpus for text_data\n",
    "text_data = df[\"clean_body_no_stopwords\"].tolist()\n",
    "\n",
    "# Convert text_data (type: list of string) to a list of lists of strings by\n",
    "# splitting each string in the original text_data\n",
    "# Note: text_data becomes a list of lists of strings where\n",
    "#       each inner list represents a document, and is a list of strings with\n",
    "#       each string being a token found in that particular document\n",
    "# Note: The length of the corpus is equal to the number of documents in the training dataframe,\n",
    "#       but the length of each inner list is dependent on each document.\n",
    "text_data = [str(text).split() for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BAG-OF-WORDS ###\n",
    "\n",
    "# Create a gensim corpora dictionary from text_data\n",
    "# where each entry in the dictionary is a unique word found in the entire corpus\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "\n",
    "# Create a bag-of-words corpus of text_data using the dictionary\n",
    "# Note: corpus is a list of lists of 2-tuples of integers where\n",
    "#       each inner list represents a document, and is a list of 2-tuples with\n",
    "#       each tuple representing a unique word found in the document, such that\n",
    "#       the first int of the tuple is the dictionary index of the word (SEE: dictionary above), and\n",
    "#       the second int of the tuple is the term frequency of the corresponding word in the particular document\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "# OPTIONAL: Save the dictionary and bag-of-words corpus\n",
    "# Original save dictionary line results in error:\n",
    "# NotImplementedError: Unable to handle scheme 'c', expected one of ('', 'file', 'hdfs', 'http', 'https', 'scp', 'sftp', 'ssh', 'webhdfs'). Extra dependencies required by 'c' may be missing. See <https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst> for details.\n",
    "#dictionary.save(PROJECT_FOLDER + \"dictionary.gensim\")\n",
    "# Following line to save dictionary saves it to cwd.\n",
    "dictionary.save(\"dictionary.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LATENT DIRICHLET ALLOCATION (LDA) ###\n",
    "\n",
    "# NOTE: The execution of LDA may take some time depending on input size.\n",
    "\n",
    "# Define k topics for LDA\n",
    "NUM_TOPICS = 8\n",
    "\n",
    "# Make the LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word = dictionary, passes = 15)\n",
    "\n",
    "# Save the LDA model\n",
    "# Original save LDA model results in error:\n",
    "# NotImplementedError: Unable to handle scheme 'c', expected one of ('', 'file', 'hdfs', 'http', 'https', 'scp', 'sftp', 'ssh', 'webhdfs'). Extra dependencies required by 'c' may be missing. See <https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst> for details.\n",
    "#dictionary.save(PROJECT_FOLDER + \"dictionary.gensim\")\n",
    "# Following line to save LDA model saves it to cwd.\n",
    "#ldamodel.save(PROJECT_FOLDER + \"LDAmodel\" + str(NUM_TOPICS) + \".gensim\")\n",
    "ldamodel.save(\"LDAmodel\" + str(NUM_TOPICS) + \".gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMINE TOPICS ###\n",
    "\n",
    "def print_LDA_topics_with_words(ldamodel, num_words):\n",
    "    topics = ldamodel.print_topics(NUM_TOPICS, num_words)\n",
    "    for topic in topics:\n",
    "        print(\"Topic #\" + str(topic[0]))\n",
    "        words = topic[1].split(\" + \")\n",
    "        print(\"Words in Topic:\")\n",
    "        w_counter = 1\n",
    "        for word in words:\n",
    "            print(\"\\tWord #\" + str(w_counter) + \": \" + word)\n",
    "            w_counter += 1\n",
    "        print()\n",
    "\n",
    "print_LDA_topics_with_words(ldamodel, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### LDA VISUALIZATION ###\n",
    "\n",
    "lda_display = pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary, sort_topics = False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE LDA HTML OUTPUT ###\n",
    "\n",
    "pyLDAvis.save_html(lda_display, (PROJECT_FOLDER + \"LDAdisplay\" + str(NUM_TOPICS) +  \".html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### MAKE LDA OUTPUT FILE FOR TOPIC DISTRIBUTIONS FOR EACH DOC ###\n",
    "\n",
    "BoW_reps = []\n",
    "topic_dist_lists = []\n",
    "\n",
    "for doc in corpus:\n",
    "    BoW_reps.append(doc)\n",
    "    topic_dist_vector = ldamodel[doc]\n",
    "    topic_dist_lists.append(topic_dist_vector)\n",
    "    print(\". \", end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload .csv file and put data in dataframe\n",
    "output_df = pd.read_csv((PROJECT_FOLDER + FILENAME + \".csv\"), header=0, encoding='utf-8')\n",
    "\n",
    "output_df = output_df.assign(BoW_Representation = BoW_reps)\n",
    "output_df = output_df.assign(Topic_Distribution = topic_dist_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv((PROJECT_FOLDER + FILENAME + \"_LDA_output.csv\"), index=None, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARSE EACH TOPIC DISTRIBUTION LIST FOR EACH DOC ###\n",
    "\n",
    "def init_TopicProbLists(num_docs):\n",
    "    init_list = [0 for x in range(0, num_docs)]\n",
    "    topic_lists = [copy.deepcopy(init_list) for x in range(0, NUM_TOPICS)]\n",
    "    return topic_lists\n",
    "\n",
    "def parse_TD_list(topic_lists, doc_counter, TD_list):\n",
    "    for topic_prob_pair in TD_list:\n",
    "        topic_num = topic_prob_pair[0]\n",
    "        probability = topic_prob_pair[1]\n",
    "        #print(\"Topic #\" + str(topic_num) + \" probability = \" + str(probability))\n",
    "        #print(\"\\t Accessing... topic_lists[\" + str(topic_num) + \"][\" + str(doc_counter) + \"]\")\n",
    "        topic_lists[topic_num][doc_counter] = probability\n",
    "        #for T_list in topic_lists:\n",
    "        #    print(T_list)\n",
    "    return topic_lists\n",
    "        \n",
    "def parse_TopicDistLists(topic_dist_lists, topic_lists):\n",
    "    doc_counter = 0\n",
    "    for TD_list in topic_dist_lists:\n",
    "        #print((20 * \"-\"), (\"Doc #\" + str(doc_counter)), (20 * \"-\"))\n",
    "        topic_lists = parse_TD_list(topic_lists, doc_counter, TD_list)\n",
    "        doc_counter += 1\n",
    "        #print()\n",
    "        #print(\". \", end = \"\")\n",
    "    print()\n",
    "    return topic_lists\n",
    "\n",
    "topic_lists = init_TopicProbLists(10)\n",
    "topic_lists = parse_TopicDistLists(topic_dist_lists[0:10], topic_lists)\n",
    "\n",
    "print((20 * \"-\"), \"END RESULT\", (20 * \"-\"))\n",
    "for T_list in topic_lists:\n",
    "    print(T_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_lists = init_TopicProbLists(len(topic_dist_lists))\n",
    "topic_lists = parse_TopicDistLists(topic_dist_lists, topic_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(topic_dist_lists))\n",
    "print(topic_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, NUM_TOPICS):\n",
    "    for j in range(550, 560):\n",
    "        print(topic_lists[i][j], \", \", end = \"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = output_df.assign(Topic_1 = topic_lists[0])\n",
    "output_df = output_df.assign(Topic_2 = topic_lists[1])\n",
    "output_df = output_df.assign(Topic_3 = topic_lists[2])\n",
    "output_df = output_df.assign(Topic_4 = topic_lists[3])\n",
    "output_df = output_df.assign(Topic_5 = topic_lists[4])\n",
    "output_df = output_df.assign(Topic_6 = topic_lists[5])\n",
    "output_df = output_df.assign(Topic_7 = topic_lists[6])\n",
    "output_df = output_df.assign(Topic_8 = topic_lists[7])\n",
    "#output_df = output_df.assign(Topic_9 = topic_lists[8])\n",
    "#output_df = output_df.assign(Topic_10 = topic_lists[9])\n",
    "#output_df = output_df.assign(Topic_11 = topic_lists[10])\n",
    "#output_df = output_df.assign(Topic_12 = topic_lists[11])\n",
    "#output_df = output_df.assign(Topic_13 = topic_lists[12])\n",
    "#output_df = output_df.assign(Topic_14 = topic_lists[13])\n",
    "#output_df = output_df.assign(Topic_15 = topic_lists[14])\n",
    "#output_df = output_df.assign(Topic_16 = topic_lists[15])\n",
    "#output_df = output_df.assign(Topic_17 = topic_lists[16])\n",
    "#output_df = output_df.assign(Topic_18 = topic_lists[17])\n",
    "#output_df = output_df.assign(Topic_19 = topic_lists[18])\n",
    "#output_df = output_df.assign(Topic_20 = topic_lists[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv((PROJECT_FOLDER + FILENAME + \"_LDA_output_parsed.csv\"), index=None, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE LDA TOPICS AND WORDS OUTPUT FILE ###\n",
    "\n",
    "def make_LDA_topics_dict(ldamodel, num_words):\n",
    "    topics = ldamodel.print_topics(NUM_TOPICS, num_words)\n",
    "    topics_dict = {\n",
    "        \"Topic\": [],\n",
    "        \"Top 30 Terms\": []\n",
    "    }\n",
    "    for topic in topics:\n",
    "        t_num = \"Topic \" + str(topic[0])\n",
    "        topics_dict[\"Topic\"].append(t_num)\n",
    "        words = topic[1].split(\" + \")\n",
    "        topics_dict[\"Top 30 Terms\"].append(words)\n",
    "    return topics_dict\n",
    "\n",
    "topics_dict = make_LDA_topics_dict(ldamodel, 30)\n",
    "topics_df = pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.to_csv((PROJECT_FOLDER + FILENAME + \"_LDA_topics.csv\"), index=None, header=True, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
